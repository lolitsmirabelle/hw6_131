---
title: "Homework 6"
output: html_document
date: '2022-05-24'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(ggplot2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(ranger)
```

## Question 1

```{r}
pokemon_og <- read.csv("~/Downloads/homework-6/data/pokemon.csv")
pokemon <- clean_names(pokemon_og)

classes <- c('Bug', 'Fire', 'Grass', 'Normal', 'Water', 'Psychic')
pokemon <- filter(pokemon, type_1 %in% classes)
pokemon$type_1 <- factor(pokemon$type_1)
pokemon$legendary <- factor(pokemon$legendary)

set.seed(2424)
pokemon_split <- initial_split(pokemon, prop = 0.80,
                                strata = type_1)
pokemon_train <- training(pokemon_split) 
pokemon_test <- testing(pokemon_split)
pokemon_folds <- vfold_cv(pokemon_train, v = 5)

pokemon_recipe <- recipe(type_1 ~ ., data = pokemon%>%dplyr::select(type_1,hp:legendary))%>%
  step_dummy(c("legendary", "generation"))%>%
  step_center(all_predictors())%>%
  step_scale(all_predictors())
```


## Question 2

```{r}
cor_pokemon_train <- pokemon_train %>%
  select(-c(name, type_1, type_2, legendary)) %>%
  correlate()
rplot(cor_pokemon_train)
```

For the correlation matrix, I removed all of the non-numeric variables (name, type_1, type_2, and legendary). From the above correlation matrix, we can see that there are multiple positive relationships:

* generation and x
* total  to speed, sp_def, sp_atk, defense, attack and hp
* hp to attack, defense, total, sp_atk, and sp_def
* attack to speed, sp_def, sp_atk, defense, hp, and total
* defense to sp_def, sp_atk, attack, hp, and total
* sp_atk to speed, sp_def, defense, attack, hp, and total
* sp_def to sp_atk, defense, attack, hp, and total
* speed to sp_atk, attack and total


## Question 3

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune()))%>%
  add_recipe(pokemon_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = pokemon_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)
```

What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?
From this, we can see that a single decision tree perfforms better with a smaller complexity penalty.


## Question 4
```{r}
a <- collect_metrics(tune_res)

arrange(a,cost_complexity)

```

The best roc_auc from the best performing ppruned decision tree on the folds is 0.6541441. 

## Question 5
```{r}
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

```{r}
pokemon_spec <- rand_forest(mtry = .cols()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

pokemon_wf <- workflow() %>%
  add_model(pokemon_spec %>% set_args(mtry = tune()) %>% set_args(trees = tune())  %>% set_args(min_n = tune())) %>%
  add_recipe(pokemon_recipe)

pokemon_fit <- fit(pokemon_spec, type_1 ~ ., 
                   data = pokemon_train)
```

* mtry is the number of predictors that will be randomly sampled at each split when creating the tree models.

* tree is the number of trees contained in the ensemble.

* min_n is the minimum number of data points in a node that are required for the node to be split further.

```{r}
forest_grid <- grid_regular(mtry(range = c(1, 8)), trees(range = c(1, 8)), min_n(range = c(1, 8)), levels = 8)
```

mtry should not be smaller than 1 or more than 8 because we do not have more than predictors that will be randomly sampled at each split when creating the tree models. mtry = 8 represents a


## Question 6
```{r}
tune_forest_res <- tune_grid(
  pokemon_wf, 
  resamples = pokemon_folds, 
  grid = forest_grid, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_forest_res)
```


## Question 7
```{r}
b <- collect_metrics(tune_forest_res)

arrange(b, mean)
```


## Question 8
```{r}
best_for <- select_best(tune_forest_res)

forest_final <- finalize_workflow(pokemon_wf, best_for)

forest_final_fit <- fit(forest_final, data = pokemon_train)

forest_final_fit%>%
  pull_workflow_fit()%>%
  vip()
```


## Question 9
```{r}

```


## Question 10
```{r}

```

